{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc9be56",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b3b52",
   "metadata": {},
   "source": [
    "In this exercise we use the IMDb-dataset, which we will use to perform a sentiment analysis. The code below assumes that the data is placed in the same folder as this notebook. We see that the reviews are loaded as a pandas dataframe, and print the beginning of the first few reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67da3bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                                   0\n",
      "0  bromwell high is a cartoon comedy . it ran at ...\n",
      "1  story of a man who has unnatural feelings for ...\n",
      "2  homelessness  or houselessness as george carli...\n",
      "3  airport    starts as a brand new luxury    pla...\n",
      "4  brilliant over  acting by lesley ann warren . ...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "reviews = pd.read_csv('reviews.txt', header=None)\n",
    "labels = pd.read_csv('labels.txt', header=None)\n",
    "Y = (labels=='positive').astype(np.int_)\n",
    "\n",
    "print(type(reviews))\n",
    "print(reviews.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982b946",
   "metadata": {},
   "source": [
    "**(a)** Split the reviews and labels in test, train and validation sets. The train and validation sets will be used to train your model and tune hyperparameters, the test set will be saved for testing. Use the `CountVectorizer` from `sklearn.feature_extraction.text` to create a Bag-of-Words representation of the reviews. Only use the 10,000 most frequent words (use the `max_features`-parameter of `CountVectorizer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70d67c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 16000\n",
      "Validation set size: 4000\n",
      "Test set size: 5000\n",
      "\n",
      "Vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    reviews, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "\n",
    "X_train_bow = vectorizer.fit_transform(X_train[0])\n",
    "X_val_bow = vectorizer.transform(X_val[0])\n",
    "X_test_bow = vectorizer.transform(X_test[0])\n",
    "\n",
    "print(f\"\\nVocabulary size: {len(vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf07ee9",
   "metadata": {},
   "source": [
    "**(b)** Explore the representation of the reviews. How is a single word represented? How about a whole review?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa3b667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Word Representation\n",
      "The word 'good' is represented by index: 3845\n",
      "This means it corresponds to column 3845 in the BoW matrix\n",
      "\n",
      "More word examples:\n",
      "'movie' -> index 5850\n",
      "'bad' -> index 668\n",
      "'excellent' -> index 3097\n",
      "'terrible' -> index 8925\n",
      "\n",
      " Whole Review Representation\n",
      "Original review text (first 200 chars):\n",
      "6911     the     michael keaton kiddie comedy of the sa...\n",
      "17960    beautiful film  pure cassavetes style . gena r...\n",
      "13714    i wonder what audiences of the day thought whe...\n",
      "15813    leonard maltin compared this film to a mel bro...\n",
      "22169    let me start out by saying i can enjoy just ab...\n",
      "                               ...                        \n",
      "4074     i love horses and admire hand drawn animation ...\n",
      "21832    hollow point  though clumsy in places  manages...\n",
      "16953    the good earth is not a great film by any mean...\n",
      "4232     william castle is notorious among horror fans ...\n",
      "15374    cunningly interesting western from a director ...\n",
      "Name: 0, Length: 200, dtype: object...\n",
      "\n",
      "BoW representation shape: (1, 10000)\n",
      "Type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Number of non-zero elements: 66\n",
      "\n",
      "Non-zero word counts in this review:\n",
      "Total unique words from vocabulary: 66\n"
     ]
    }
   ],
   "source": [
    "# 1. How is a single word represented?\n",
    "print(\"Single Word Representation\")\n",
    "word_example = \"good\"\n",
    "if word_example in vectorizer.vocabulary_:\n",
    "    word_index = vectorizer.vocabulary_[word_example]\n",
    "    print(f\"The word '{word_example}' is represented by index: {word_index}\")\n",
    "    print(f\"This means it corresponds to column {word_index} in the BoW matrix\")\n",
    "else:\n",
    "    print(f\"'{word_example}' not in vocabulary\")\n",
    "\n",
    "# Show a few more examples\n",
    "print(\"\\nMore word examples:\")\n",
    "for word in [\"movie\", \"bad\", \"excellent\", \"terrible\"]:\n",
    "    if word in vectorizer.vocabulary_:\n",
    "        print(f\"'{word}' -> index {vectorizer.vocabulary_[word]}\")\n",
    "\n",
    "# 2. How is a whole review represented?\n",
    "print(\"\\n Whole Review Representation\")\n",
    "review_index = 0\n",
    "print(f\"Original review text (first 200 chars):\\n{X_train[review_index][:200]}...\\n\")\n",
    "\n",
    "# Get the BoW representation\n",
    "review_bow = X_train_bow[review_index]\n",
    "print(f\"BoW representation shape: {review_bow.shape}\")\n",
    "print(f\"Type: {type(review_bow)}\")\n",
    "print(f\"Number of non-zero elements: {review_bow.nnz}\")\n",
    "\n",
    "review_dense = review_bow.toarray().flatten()\n",
    "non_zero_indices = np.where(review_dense > 0)[0]\n",
    "print(f\"\\nNon-zero word counts in this review:\")\n",
    "print(f\"Total unique words from vocabulary: {len(non_zero_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6dc2fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example word counts:\n",
      "  'and' appears 2 times\n",
      "  'arms' appears 1 times\n",
      "  'as' appears 1 times\n",
      "  'be' appears 1 times\n",
      "  'boggling' appears 1 times\n",
      "  'but' appears 2 times\n",
      "  'cast' appears 1 times\n",
      "  'comedy' appears 2 times\n",
      "  'compared' appears 1 times\n",
      "  'condemned' appears 1 times\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExample word counts:\")\n",
    "idx_to_word = {i: w for w, i in vectorizer.vocabulary_.items()} #reverse mapping\n",
    "    \n",
    "for idx in non_zero_indices[:10]: \n",
    "    word = idx_to_word[idx] # get word from index\n",
    "    count = review_dense[idx] \n",
    "    print(f\"  '{word}' appears {int(count)} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2638fce",
   "metadata": {},
   "source": [
    "**(c)** Train a neural network with a single hidden layer on the dataset, tuning the relevant hyperparameters to optimize accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec15e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (16000, 10000)\n",
      "Trying units=32, dropout=0.0, lr=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mtymi\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  val_acc=0.8835\n",
      "Trying units=32, dropout=0.0, lr=0.0001\n",
      "  val_acc=0.8947\n",
      "Trying units=32, dropout=0.1, lr=0.001\n",
      "  val_acc=0.8832\n",
      "Trying units=32, dropout=0.1, lr=0.0001\n",
      "  val_acc=0.8947\n",
      "Trying units=32, dropout=0.2, lr=0.001\n",
      "  val_acc=0.8875\n",
      "Trying units=32, dropout=0.2, lr=0.0001\n",
      "  val_acc=0.8947\n",
      "Trying units=64, dropout=0.0, lr=0.001\n",
      "  val_acc=0.8840\n",
      "Trying units=64, dropout=0.0, lr=0.0001\n",
      "  val_acc=0.8957\n",
      "Trying units=64, dropout=0.1, lr=0.001\n",
      "  val_acc=0.8873\n",
      "Trying units=64, dropout=0.1, lr=0.0001\n",
      "  val_acc=0.8967\n",
      "Trying units=64, dropout=0.2, lr=0.001\n",
      "  val_acc=0.8842\n",
      "Trying units=64, dropout=0.2, lr=0.0001\n",
      "  val_acc=0.8965\n",
      "Trying units=128, dropout=0.0, lr=0.001\n",
      "  val_acc=0.8852\n",
      "Trying units=128, dropout=0.0, lr=0.0001\n",
      "  val_acc=0.8953\n",
      "Trying units=128, dropout=0.1, lr=0.001\n",
      "  val_acc=0.8865\n",
      "Trying units=128, dropout=0.1, lr=0.0001\n",
      "  val_acc=0.8975\n",
      "Trying units=128, dropout=0.2, lr=0.001\n",
      "  val_acc=0.8817\n",
      "Trying units=128, dropout=0.2, lr=0.0001\n",
      "  val_acc=0.8967\n",
      "Trying units=256, dropout=0.0, lr=0.001\n",
      "  val_acc=0.8845\n",
      "Trying units=256, dropout=0.0, lr=0.0001\n",
      "  val_acc=0.8965\n",
      "Trying units=256, dropout=0.1, lr=0.001\n",
      "  val_acc=0.8848\n",
      "Trying units=256, dropout=0.1, lr=0.0001\n",
      "  val_acc=0.8970\n",
      "Trying units=256, dropout=0.2, lr=0.001\n",
      "  val_acc=0.8867\n",
      "Trying units=256, dropout=0.2, lr=0.0001\n",
      "  val_acc=0.8942\n",
      "\n",
      "Best config (from quick sweep): {'units': 128, 'dropout': 0.1, 'lr': 0.0001} val_acc: 0.8974999785423279\n",
      "\n",
      "Training final model on train+val...\n",
      "Epoch 1/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 47ms/step - accuracy: 0.7960 - loss: 0.4940\n",
      "Epoch 2/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.8935 - loss: 0.3060\n",
      "Epoch 3/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9182 - loss: 0.2419\n",
      "Epoch 4/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9324 - loss: 0.2041\n",
      "Epoch 5/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9429 - loss: 0.1764\n",
      "Epoch 6/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9512 - loss: 0.1552\n",
      "Epoch 7/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9586 - loss: 0.1368\n",
      "Epoch 8/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - accuracy: 0.9649 - loss: 0.1220\n",
      "Epoch 9/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9709 - loss: 0.1085\n",
      "Epoch 10/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9749 - loss: 0.0971\n",
      "Epoch 11/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9797 - loss: 0.0861\n",
      "Epoch 12/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9814 - loss: 0.0781\n",
      "Epoch 13/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9844 - loss: 0.0694\n",
      "Epoch 14/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9875 - loss: 0.0618\n",
      "Epoch 15/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9895 - loss: 0.0558\n",
      "Epoch 16/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9912 - loss: 0.0490\n",
      "Epoch 17/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.9926 - loss: 0.0438\n",
      "Epoch 18/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9940 - loss: 0.0391\n",
      "Epoch 19/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9948 - loss: 0.0343\n",
      "Epoch 20/20\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.9958 - loss: 0.0302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2256a359ca0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "X_train_dense = X_train_bow.toarray()\n",
    "X_val_dense = X_val_bow.toarray()\n",
    "X_test_dense = X_test_bow.toarray()\n",
    "\n",
    "print(f\"X_train shape: {X_train_dense.shape}\")\n",
    "\n",
    "# Prepare dense matrices (already created earlier) and labels\n",
    "# X_train_dense, X_val_dense, X_test_dense are expected to exist from previous cells\n",
    "y_train_bin = (y_train[0] == 'positive').astype(int).values\n",
    "y_val_bin = (y_val[0]   == 'positive').astype(int).values\n",
    "y_test_bin = (y_test[0]  == 'positive').astype(int).values\n",
    "\n",
    "input_dim = X_train_dense.shape[1]\n",
    "\n",
    "def build_model(units=64, dropout=0.2, lr=1e-3):\n",
    "    model = keras.Sequential([\n",
    "        layers.InputLayer(input_shape=(input_dim,)),\n",
    "        layers.Dense(units, activation='relu'),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Quick hyperparameter sweep (short runs to pick promising config)\n",
    "param_grid = {\n",
    "    \"units\": [32, 64, 128, 256],\n",
    "    \"dropout\": [0.0, 0.1, 0.2],\n",
    "    \"lr\": [1e-3, 1e-4]\n",
    "}\n",
    "\n",
    "best_cfg = None\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for units in param_grid[\"units\"]:\n",
    "    for dropout in param_grid[\"dropout\"]:\n",
    "        for lr in param_grid[\"lr\"]:\n",
    "            print(f\"Trying units={units}, dropout={dropout}, lr={lr}\")\n",
    "            model = build_model(units=units, dropout=dropout, lr=lr)\n",
    "            hist = model.fit(\n",
    "                X_train_dense, y_train_bin,\n",
    "                validation_data=(X_val_dense, y_val_bin),\n",
    "                epochs=5, batch_size=64, verbose=0\n",
    "            )\n",
    "            val_acc = hist.history['val_accuracy'][-1]\n",
    "            print(f\"  val_acc={val_acc:.4f}\")\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_cfg = {\"units\": units, \"dropout\": dropout, \"lr\": lr}\n",
    "\n",
    "print(\"\\nBest config (from quick sweep):\", best_cfg, \"val_acc:\", best_val_acc)\n",
    "\n",
    "# Final training: combine train+val and train a final model with the chosen config\n",
    "X_combined = np.vstack([X_train_dense, X_val_dense])\n",
    "y_combined = np.concatenate([y_train_bin, y_val_bin])\n",
    "\n",
    "final_units = best_cfg[\"units\"]\n",
    "final_dropout = best_cfg[\"dropout\"]\n",
    "final_lr = best_cfg[\"lr\"]\n",
    "\n",
    "final_model = build_model(units=final_units, dropout=final_dropout, lr=final_lr)\n",
    "\n",
    "es = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "print(\"\\nTraining final model on train+val...\")\n",
    "final_model.fit(\n",
    "    X_combined, y_combined,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd327a6",
   "metadata": {},
   "source": [
    "**(d)** Test your sentiment-classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fff468cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set -> loss: 0.5220, accuracy: 0.8694\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = final_model.evaluate(X_test_dense, y_test_bin, verbose=0)\n",
    "print(f\"\\nTest set -> loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd44ee62",
   "metadata": {},
   "source": [
    "**(e)** Use the classifier to classify a few sentences you write yourselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1ef2970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\n",
      "Sentence: I loved this movie. It was exciting and very well acted.\n",
      "  prob_positive=0.910 -> positive\n",
      "\n",
      "Sentence: This was a terrible movie. I wasted my time.\n",
      "  prob_positive=0.141 -> negative\n",
      "\n",
      "Sentence: The plot was okay but the acting saved it.\n",
      "  prob_positive=0.183 -> negative\n",
      "\n",
      "Sentence: Boring, predictable and too long.\n",
      "  prob_positive=0.102 -> negative\n",
      "\n",
      "Sentence: I was having high hopes, but it just didn't deliver.\n",
      "  prob_positive=0.532 -> positive\n",
      "\n",
      "Sentence: Watching this film was one of the biggest mistakes of my life.\n",
      "  prob_positive=0.438 -> negative\n",
      "\n",
      "Sentence: I could find better ways to spend my time than watching this.\n",
      "  prob_positive=0.397 -> negative\n",
      "\n",
      "Sentence: Very good way to waste time.\n",
      "  prob_positive=0.216 -> negative\n",
      "\n",
      "Sentence: This sentence has nothing to do with movies. Just wanted to see what happens.\n",
      "  prob_positive=0.211 -> negative\n",
      "\n",
      "Sentence: Great, amazing, spectacular, decent.\n",
      "  prob_positive=0.853 -> positive\n"
     ]
    }
   ],
   "source": [
    "custom_sentences = [\n",
    "    \"I loved this movie. It was exciting and very well acted.\",\n",
    "    \"This was a terrible movie. I wasted my time.\",\n",
    "    \"The plot was okay but the acting saved it.\",\n",
    "    \"Boring, predictable and too long.\",\n",
    "    \"I was having high hopes, but it just didn't deliver.\",\n",
    "    \"Watching this film was one of the biggest mistakes of my life.\",\n",
    "    \"I could find better ways to spend my time than watching this.\",\n",
    "    \"Very good way to waste time.\",\n",
    "    \"This sentence has nothing to do with movies. Just wanted to see what happens.\",\n",
    "    \"Great, amazing, spectacular, decent.\"\n",
    "]\n",
    "\n",
    "# Vectorize and predict\n",
    "X_custom_bow = vectorizer.transform(custom_sentences)\n",
    "preds = final_model.predict(X_custom_bow.toarray()).flatten()\n",
    "for sent, p in zip(custom_sentences, preds):\n",
    "    label = 'positive' if p >= 0.5 else 'negative'\n",
    "    print(f\"\\nSentence: {sent}\\n  prob_positive={p:.3f} -> {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793c955d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
